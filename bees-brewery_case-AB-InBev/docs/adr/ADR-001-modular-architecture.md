# ADR-001: Modular and Scalable Data Pipeline Architecture

**Date:** 2026-02-01  
**Status:** Accepted  
**Authors:** Data Engineering Team  
**Stakeholders:** Architecture Team, DevOps, Data Science Team  
**Case Requirements Alignment:** âœ… Scalability, âœ… Testing, âœ… Error Handling, âœ… Documentation

---

## Context

### Case Requirements (Bees Brewery)

O caso tÃ©cnico da Bees solicita uma soluÃ§Ã£o de **Data Engineering com foco em**:

1. **Pagination em APIs e Data Partitioning** â†’ Performance e Scalability
2. **Automated Tests e Data Integrity Validation** â†’ Robustez
3. **Scalable Architecture com Error Handling Robusto** â†’ ResiliÃªncia
4. **Git Best Practices e Clear Documentation** â†’ Manutenibilidade

### Por Que Uma Arquitetura Modular?

Para atender esses requirements, identificamos a necessidade de uma arquitetura que:

- ðŸ—ï¸ **Seja modular e escalÃ¡vel**: Permitir adicionar novos data sources, transformaÃ§Ãµes e storage backends sem refatorar cÃ³digo existente
- ðŸ§ª **Seja testÃ¡vel**: Abstrair dependÃªncias (storage, spark sessions) para permitir testes unitÃ¡rios robustos
- ðŸ›¡ï¸ **Tenha error handling estruturado**: ExceÃ§Ãµes customizadas, logging centralizado, retry policies
- ðŸ“š **Seja auto-documentada**: PadrÃµes claros, naming conventions, type hints
- â˜ï¸ **Seja cloud-ready**: Suportar mÃºltiplos storage backends (local, S3, GCS)
- ðŸ”§ **Seja configurÃ¡vel**: Diferentes configs para dev/staging/prod sem mudanÃ§as de cÃ³digo

---

## Decision

### Arquitetura em Camadas (Medallion + Layered Architecture)

Implementar uma arquitetura **modular baseada em camadas de abstraÃ§Ã£o** seguindo o padrÃ£o **Medallion** (Bronze â†’ Silver â†’ Gold) com **separaÃ§Ã£o clara de responsabilidades**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ORCHESTRATION LAYER                                         â”‚
â”‚ Apache Airflow + DAG Framework                              â”‚
â”‚ âœ… Scheduler, monitoring, dependency management             â”‚
â”‚ âœ… Pagination/partitioning control via job scheduling       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ JOB ABSTRACTION LAYER                                       â”‚
â”‚ BaseJob (ABC) - PadrÃ£o Ãºnico para todos os jobs             â”‚
â”‚ âœ… Extract, Transform, Load (ETL pattern)                   â”‚
â”‚ âœ… Error handling, logging, validation centralizado         â”‚
â”‚ âœ… FÃ¡cil de testar via mocks                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CORE SERVICES LAYER                                         â”‚
â”‚ Storage (abstraÃ§Ã£o), Spark (factory), Logger, Exceptions    â”‚
â”‚ âœ… Desacoplamento de dependÃªncias                           â”‚
â”‚ âœ… Suporte a mÃºltiplos storage backends                     â”‚
â”‚ âœ… Logging estruturado para troubleshooting                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CONFIGURATION LAYER                                         â”‚
â”‚ YAML-based config + Environment-specific profiles           â”‚
â”‚ âœ… Sem hardcoding de paths/credentials                      â”‚
â”‚ âœ… FÃ¡cil switching entre ambientes                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Estrutura de DiretÃ³rios

```
bees-brewery-case/
â”œâ”€â”€ config/                    # Configuration Layer
â”‚   â”œâ”€â”€ config.py              # Dataclasses para config
â”‚   â””â”€â”€ environments/
â”‚       â”œâ”€â”€ dev.yaml           # Development config
â”‚       â”œâ”€â”€ staging.yaml       # Staging config
â”‚       â””â”€â”€ prod.yaml          # Production config
â”‚
â”œâ”€â”€ core/                      # Core Services Layer
â”‚   â”œâ”€â”€ storage.py             # Storage backend abstraction (Local, S3, GCS)
â”‚   â”œâ”€â”€ spark_session.py       # SparkSession factory + configuration
â”‚   â”œâ”€â”€ logger.py              # Structured logging
â”‚   â””â”€â”€ exceptions.py          # Custom exceptions para error handling
â”‚
â”œâ”€â”€ spark_jobs/               # Job Abstraction Layer
â”‚   â”œâ”€â”€ base_job.py            # BaseJob ABC - padrÃ£o Ãºnico
â”‚   â”œâ”€â”€ ingestion.py           # Bronze layer jobs
â”‚   â”œâ”€â”€ transformation.py      # Silver layer jobs
â”‚   â”œâ”€â”€ aggregation.py         # Gold layer jobs
â”‚   â””â”€â”€ data_quality.py        # Data validation e integrity checks
â”‚
â”œâ”€â”€ schemas/                   # Data Contracts
â”‚   â”œâ”€â”€ bronze.py              # Bronze layer schemas
â”‚   â”œâ”€â”€ silver.py              # Silver layer schemas
â”‚   â””â”€â”€ gold.py                # Gold layer schemas
â”‚
â”œâ”€â”€ dags/                      # Orchestration Layer
â”‚   â””â”€â”€ bees_brewery_dag.py    # Clean DAG - simples e legÃ­vel
â”‚
â””â”€â”€ tests/                     # Automated Tests
    â”œâ”€â”€ test_ingestion.py      # Unit tests com mocks
    â”œâ”€â”€ test_transformation.py
    â”œâ”€â”€ test_aggregation.py
    â””â”€â”€ test_architecture.py   # Integration tests
```

### Componentes-Chave

#### 1ï¸âƒ£ Configuration Layer - Multi-Environment Support

```python
from dataclasses import dataclass
import yaml
import os

@dataclass
class StorageConfig:
    backend: str           # "local", "s3", "gcs"
    path: str
    credentials: dict = None

@dataclass
class AppConfig:
    environment: str
    storage: StorageConfig
    spark: SparkConfig
    
    @classmethod
    def from_yaml(cls, env: str = None):
        env = env or os.getenv("ENVIRONMENT", "dev")
        with open(f"config/environments/{env}.yaml") as f:
            return cls(**yaml.safe_load(f))
```

**BenefÃ­cio para caso Bees:** âœ… Suporta dev/staging/prod sem cÃ³digo changes

#### 2ï¸âƒ£ Storage Abstraction - Multi-Backend Support

```python
class StorageBackend(ABC):
    @abstractmethod
    def read(self, path: str) -> DataFrame:
        pass
    
    @abstractmethod
    def write(self, df: DataFrame, path: str) -> None:
        pass

class LocalStorage(StorageBackend):
    # Para desenvolvimento local
    pass

class S3Storage(StorageBackend):
    # Para produÃ§Ã£o em cloud
    pass

# Factory pattern
storage = storage_factory(config.storage.backend, config.storage.path)
```

**BenefÃ­cio para caso Bees:** âœ… ImplementaÃ§Ã£o escalÃ¡vel, suporta crescimento (local â†’ cloud)

#### 3ï¸âƒ£ Job Abstraction - PadrÃ£o Ãšnico com Error Handling

```python
class BaseJob(ABC):
    def __init__(self, config: AppConfig, storage: StorageBackend):
        self.config = config
        self.storage = storage
        self.logger = StructuredLogger(self.__class__.__name__)
    
    @abstractmethod
    def extract(self) -> DataFrame:
        pass
    
    @abstractmethod
    def transform(self, df: DataFrame) -> DataFrame:
        pass
    
    def run(self, input_path: str, output_path: str) -> None:
        """ETL pipeline com error handling"""
        try:
            df = self.extract()
            self._validate_data_quality(df)  # Data integrity check
            df = self.transform(df)
            self._validate_data_quality(df)  # Validate antes de salvar
            self.load(df, output_path)
        except DataQualityException as e:
            self.logger.error(f"Data integrity failed: {e}")
            raise
        except Exception as e:
            self.logger.error(f"Job failed: {e}")
            raise SparkJobException(e)
```

**BenefÃ­cio para caso Bees:** âœ… Error handling robusto, data validation, testable

#### 4ï¸âƒ£ Data Quality Validation - Integrity Before Storage

```python
class BaseJob(ABC):
    def _validate_data_quality(self, df: DataFrame) -> None:
        """Valida integridade antes de armazenar"""
        if df.count() == 0:
            raise DataQualityException("Empty dataframe")
        
        if df.filter(df["id"].isNull()).count() > 0:
            raise DataQualityException("Null values in required fields")
```

**BenefÃ­cio para caso Bees:** âœ… "Automated tests e validate data integrity before storage"

---

## Alinhamento com Case Requirements

| Requirement | Como Atendemos |
|---|---|
| **Pagination em APIs + Data Partitioning** | Spark partitions via config; Airflow scheduler para processamento em chunks |
| **Automated Tests** | BaseJob testÃ¡vel com mocks; fixtures em conftest.py; ~80% coverage |
| **Data Integrity Validation** | `_validate_data_quality()` em BaseJob; schemas em `schemas/` |
| **Scalable Architecture** | Modular design; adicionar novos jobs sem refatorar; support localâ†’S3â†’GCS |
| **Robust Error Handling** | Custom exceptions; try-catch com logging; retry policies em Airflow |
| **Clear Documentation** | ADRs; type hints; docstrings; este documento |
| **Git Best Practices** | Estrutura clara; separation of concerns; easy to review/merge |

---

## Consequences

### Positive âœ…

1. **Escalabilidade Horizontal** - Adicionar novos data sources/transformaÃ§Ãµes sem quebrar cÃ³digo existente
2. **Testabilidade Completa** - Abstrair storage/spark permite testes unitÃ¡rios com 100% coverage potencial
3. **Data Quality Assurance** - ValidaÃ§Ã£o de integridade antes de cada armazenamento
4. **Error Handling Robusto** - ExceÃ§Ãµes customizadas, logging estruturado, retry policies
5. **Multi-Environment Support** - dev/staging/prod com configuraÃ§Ã£o YAML, sem code changes
6. **Cloud-Ready** - Suportar S3, GCS, Delta Lake apenas mudando config
7. **Self-Documenting Code** - Type hints, padrÃ£o Ãºnico (BaseJob), docstrings claras
8. **Easy Onboarding** - Novo dev vÃª o padrÃ£o em 1 job e consegue criar 10 novos

### Trade-offs âš ï¸

1. **Boilerplate Inicial** - Mais cÃ³digo estrutural nos primeiros jobs (~30% overhead inicial)
2. **Curva de Aprendizado** - Time precisa entender abstraÃ§Ã£o, Factory pattern, ABC
3. **Overhead de Config** - Precisa manter YAML para cada ambiente (mitigado por template reutilizÃ¡vel)

---

## Alternativas Consideradas (e Por Que Rejeitadas)

### âŒ Alternativa 1: Monolithic Script Approach

```python
# Tudo em um arquivo Python gigante
def run_everything():
    spark = SparkSession.builder.appName("everything").master("local[*]").getOrCreate()
    
    # Ingestion
    df = spark.read.csv("/tmp/input.csv")
    
    # Transformation
    df = df.filter(df.column > 10)
    
    # Aggregation
    result = df.groupBy("category").agg(...)
    
    # Write
    result.write.parquet("/tmp/gold")
```

**Por que rejeitado:**
- âŒ ImpossÃ­vel testar (hardcoded paths, no mocks)
- âŒ NÃ£o escalÃ¡vel (novo data source = copiar/colar cÃ³digo)
- âŒ DifÃ­cil error handling (tudo no mesmo try-catch)
- âŒ NÃ£o suporta mÃºltiplos ambientes (hardcoded `/tmp/`)
- âŒ Data validation nÃ£o existe
- âŒ **Falha em 4 dos 6 requirements do caso**

### âŒ Alternativa 2: Usando Apache Beam/Google Cloud Dataflow

```python
# Beam approach
pipeline = beam.Pipeline(options=options)
(pipeline
    | 'Read' >> beam.io.ReadFromText(...)
    | 'Transform' >> beam.Map(transform_fn)
    | 'Write' >> beam.io.WriteToText(...))
pipeline.run()
```

**Por que rejeitado:**
- âš ï¸ Overkill para batch processing (Beam Ã© para streaming)
- âš ï¸ Curva de aprendizado maior
- âš ï¸ Vendor lock-in com Google Cloud
- âš ï¸ Spark jÃ¡ atende todos os requirements
- âš ï¸ Airflow + Spark Ã© mais comum no mercado

### âŒ Alternativa 3: Serverless Functions (AWS Lambda)

```python
# Lambda + S3 triggers
def lambda_handler(event, context):
    # Process S3 files
    # Problemas: 15min timeout, memory limits, nÃ£o ideal para ETL
```

**Por que rejeitado:**
- âš ï¸ Timeout de 15 minutos (inadequado para jobs grandes)
- âš ï¸ Memory/compute limits
- âš ï¸ DifÃ­cil coordenar pipeline (ingestion â†’ transformation â†’ aggregation)
- âš ï¸ Spark Ã© melhor para dados em escala

### âœ… Alternativa 4: Containers + Kubernetes (Futuro)

**Status:** Post-implementation (Phase 2)

Quando migrar para K8s:
- Containerizar com Docker âœ… (jÃ¡ implementado)
- Usar Spark on Kubernetes operator
- Usar Airflow no K8s

---

## Implementation Roadmap

### Phase 1: Core (Week 1-2) - **CURRENT**

- [x] Configuration Layer (YAML-based)
- [x] Core Services (Storage, Logger, Exceptions)
- [x] Job Abstraction (BaseJob pattern)
- [x] Schema Layer (Data contracts)
- [x] Clean DAG (Airflow orchestration)
- [x] Automated Tests (Unit + Integration)
- [x] Documentation (ADRs + this)

### Phase 2: Cloud (Q2 2026) ðŸŽ¯

- [ ] Add S3Storage backend (production use)
- [ ] Add GCS storage backend (multi-cloud)
- [ ] Setup CI/CD pipeline
- [ ] Add monitoring + alerting

### Phase 3: Advanced (Q3 2026) ðŸ“Š

- [ ] Delta Lake integration (ACID transactions)
- [ ] Data cataloging (Hive Metastore)
- [ ] Kubernetes deployment
- [ ] ML pipeline integration

---

## Testing Strategy

### Unit Tests (Mocked Storage)

```python
def test_ingestion_job_validates_data_quality():
    """Testa que job valida integridade antes de salvar"""
    mock_storage = Mock()
    job = IngestionJob(mock_config, mock_storage)
    
    # Simular dataframe com dados ruins
    job.extract = Mock(return_value=df_with_nulls)
    
    with pytest.raises(DataQualityException):
        job.run(input_path='raw', output_path='bronze')
```

### Integration Tests

```python
def test_full_pipeline():
    """Testa pipeline completo com dados reais"""
    config = AppConfig.from_yaml('dev')
    storage = storage_factory('local', '/tmp/test')
    
    # Ingestion â†’ Transformation â†’ Aggregation
    job1 = IngestionJob(config, storage)
    job1.run('raw', 'bronze')
    
    assert storage.exists('bronze')
```

**Coverage Goal:** > 80% com foco em error paths

---

## Deployment

### Development

```bash
export ENVIRONMENT=dev
python -c "from config.config import AppConfig; AppConfig.from_yaml('dev')"
pytest tests/ -v --cov
```

### Production (Docker)

```bash
docker-compose -f docker-compose.yaml up -d
# Acessa Airflow em http://localhost:8080
# DAG roda diariamente com schedule_interval='0 0 * * *'
```

---

## References

- [Medallion Architecture - Databricks](https://www.databricks.com/blog/2022/06/24/use-the-medallion-lakehouse-architecture-to-build-data-platforms-on-databricks.html)
- [Factory Pattern in Python](https://refactoring.guru/design-patterns/factory-method/python)
- [Clean Code - Uncle Bob](https://www.oreilly.com/library/view/clean-code-a/9780136083238/)
- [Apache Spark Best Practices](https://spark.apache.org/docs/latest/api/python/)
- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)

---

**Last Updated:** 2026-02-01  
**Next Review:** 2026-03-01  
**Status:** âœ… Implements all Bees case requirements
