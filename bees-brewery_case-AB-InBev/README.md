# BEES Brewery Case - ETL Pipeline

Pipeline de ETL completo para processamento de dados do caso BEES Brewery, utilizando arquitetura Medallion (Bronze, Silver, Gold) com Airflow e Spark.

## ğŸ“š DocumentaÃ§Ã£o

**Leia primeiro:**
1. ğŸ“˜ **[ARCHITECTURE.md](docs/ARCHITECTURE.md)** - VisÃ£o geral da arquitetura + decisÃµes tÃ©cnicas
2. ğŸ“— **[IMPLEMENTATION.md](docs/IMPLEMENTATION.md)** - Setup, deployment e histÃ³rico de ajustes
3. ğŸ“• **[TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md)** - DiagnÃ³stico e soluÃ§Ã£o de problemas

**DecisÃµes Arquiteturais:**
- ğŸ“‹ **[ADRs Index](docs/adr/README.md)** - Architecture Decision Records
- ğŸ“Œ **[ADR-001](docs/adr/ADR-001-modular-architecture.md)** - Arquitetura Modular
- ğŸ“Œ **[ADR-002](docs/adr/ADR-002-TECH-STACK.md)** - Stack TecnolÃ³gico

---

## ğŸ“‹ Estrutura do Projeto

```
bees-brewery-case/
â”œâ”€â”€ dags/                  # DAGs do Airflow
â”‚   â””â”€â”€ bees_brewery_dag.py
â”œâ”€â”€ spark_jobs/            # Scripts PySpark segregados
â”‚   â”œâ”€â”€ ingestion.py       # API -> Bronze
â”‚   â”œâ”€â”€ transformation.py  # Bronze -> Silver
â”‚   â””â”€â”€ aggregation.py     # Silver -> Gold
â”œâ”€â”€ tests/                 # Testes unitÃ¡rios (PyTest)
â”‚   â”œâ”€â”€ test_ingestion.py
â”‚   â”œâ”€â”€ test_transformation.py
â”‚   â””â”€â”€ test_aggregation.py
â”œâ”€â”€ docker/                # Dockerfiles customizados
â”‚   â””â”€â”€ Dockerfile.spark
â”œâ”€â”€ docker-compose.yaml    # OrquestraÃ§Ã£o de containers
â””â”€â”€ requirements.txt       # DependÃªncias Python
```

## ğŸš€ Quick Start

### PrÃ©-requisitos

- Docker e Docker Compose instalados
- Python 3.9+
- Git

### 1. Clonar o repositÃ³rio

```bash
git clone <repository-url>
cd bees-brewery-case
```

### 2. Instalar dependÃªncias

```bash
pip install -r requirements.txt
```

### 3. Iniciar containers (Airflow + Spark)

```bash
docker-compose up -d
```

### 4. Acessar interfaces

- **Airflow UI**: http://localhost:8080
- **Spark Master**: http://localhost:8080

## ğŸ“š Componentes

### Ingestion (Bronze Layer)

**Arquivo**: `spark_jobs/ingestion.py`

ResponsÃ¡vel por extrair dados da API BEES e armazenar na camada Bronze em formato parquet.

**Principais mÃ©todos**:
- `fetch_data_from_api()`: Busca dados da API
- `create_bronze_dataframe()`: Cria DataFrame a partir dos dados
- `save_to_bronze()`: Salva dados em formato parquet
- `ingest()`: Executa pipeline completo

**Exemplo de uso**:
```python
from pyspark.sql import SparkSession
from spark_jobs.ingestion import BeesIngestion

spark = SparkSession.builder.appName("Ingestion").getOrCreate()
ingestion = BeesIngestion(spark, "https://api.example.com", api_key="key")
ingestion.ingest("customers", "customers", "/data")
```

### Transformation (Silver Layer)

**Arquivo**: `spark_jobs/transformation.py`

ResponsÃ¡vel por limpeza, validaÃ§Ã£o e transformaÃ§Ã£o de dados de Bronze para Silver.

**Principais mÃ©todos**:
- `clean_data()`: Remove duplicatas e normaliza strings
- `validate_data()`: Valida colunas obrigatÃ³rias e nulos
- `enrich_data()`: Adiciona metadados (timestamp, ID Ãºnico)
- `save_to_silver()`: Salva dados em Silver
- `transform()`: Executa pipeline completo

**Exemplo de uso**:
```python
from spark_jobs.transformation import BeesTransformation

transformation = BeesTransformation(spark)
df = transformation.transform(
    "/data/bronze/customers",
    "customers",
    "/data",
    required_columns=["id", "name", "email"]
)
```

### Aggregation (Gold Layer)

**Arquivo**: `spark_jobs/aggregation.py`

ResponsÃ¡vel por agregaÃ§Ãµes, anÃ¡lises e mÃ©tricas de negÃ³cio em Gold.

**Principais mÃ©todos**:
- `aggregate_by_group()`: Agrupa e aplica funÃ§Ãµes de agregaÃ§Ã£o
- `apply_filters()`: Filtra dados
- `sort_data()`: Ordena dados
- `calculate_metrics()`: Calcula mÃ©tricas customizadas
- `save_to_gold()`: Salva dados em Gold
- `aggregate()`: Executa pipeline completo

**Exemplo de uso**:
```python
from spark_jobs.aggregation import BeesAggregation

aggregation = BeesAggregation(spark)
df = aggregation.aggregate(
    "/data/silver/sales",
    "sales_summary",
    "/data",
    group_by_cols=["category"],
    agg_specs={"amount": "sum", "quantity": "avg"},
    sort_columns=[("amount", "desc")]
)
```

## ğŸ§ª Testes

Executar todos os testes:

```bash
pytest tests/ -v
```

Executar testes de um mÃ³dulo especÃ­fico:

```bash
pytest tests/test_ingestion.py -v
pytest tests/test_transformation.py -v
pytest tests/test_aggregation.py -v
```

Com cobertura:

```bash
pytest tests/ --cov=spark_jobs --cov-report=html
```

## ğŸ”„ Fluxo de Dados

```
API BEES
   â†“
[Ingestion Task] â†’ Bronze Layer (Parquet)
   â†“
[Transformation Task] â†’ Silver Layer (Parquet)
   â†“
[Aggregation Task] â†’ Gold Layer (Parquet)
   â†“
[Validation Task] â†’ Data Quality Checks
```

## ğŸ“Š Camadas Medallion

### Bronze
- Dados brutos da API
- Sem transformaÃ§Ãµes
- Formato: Parquet
- RetenÃ§Ã£o: Indefinida

### Silver
- Dados limpos e validados
- Duplicatas removidas
- Strings normalizadas
- Metadados adicionados
- Formato: Parquet

### Gold
- Dados agregados e analisados
- MÃ©tricas de negÃ³cio
- Dados prontos para BI/Analytics
- Formato: Parquet

## âš™ï¸ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

Criar arquivo `.env` na raiz do projeto:

```env
# API
API_URL=https://api.example.com
API_KEY=your-api-key

# Spark
SPARK_MASTER=spark://localhost:7077
SPARK_MEMORY=2g

# Data paths
DATA_PATH=/data

# Airflow
AIRFLOW_HOME=/opt/airflow
```

### docker-compose.yaml

Configurar services:
- Airflow Webserver
- Airflow Scheduler
- Spark Master
- Spark Worker(s)
- PostgreSQL (Airflow metadata)

## ğŸ³ Docker

### Build customizado da imagem Spark

```bash
docker build -f docker/Dockerfile.spark -t bees-spark:latest .
```

### Executar job Spark diretamente

```bash
docker exec -it bees-spark-master spark-submit \
  --class org.apache.spark.examples.SparkPi \
  /opt/spark/examples/jars/spark-examples_2.12-3.5.0.jar \
  100
```

## ğŸ“ Logs

Logs sÃ£o salvos em `/logs`:

```bash
tail -f logs/spark_jobs.log
tail -f logs/airflow_scheduler.log
```

## ğŸ¤ Contribuindo

1. Criar branch feature: `git checkout -b feature/sua-feature`
2. Commit changes: `git commit -am 'Adiciona nova feature'`
3. Push to branch: `git push origin feature/sua-feature`
4. Abrir Pull Request

## ğŸ“ Suporte

Para dÃºvidas e issues, abrir uma issue no repositÃ³rio.

## ğŸ“„ LicenÃ§a

Projeto BEES Brewery Case - Todos os direitos reservados.
